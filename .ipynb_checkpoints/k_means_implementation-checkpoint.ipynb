{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90bcd6aa",
   "metadata": {},
   "source": [
    "# Data Mining project\n",
    "\n",
    "### Clustering of countries for COVID-19 cases based on disease prevalence, health systems and environmental indicators\n",
    "\n",
    "Authors: Inga Wohlert, Nicolas Pablo Viola, Jakob Nystr√∂m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f32b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d63384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jupyter_black\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8127bfb0",
   "metadata": {},
   "source": [
    "## 1. Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8914752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_data():\n",
    "    \"\"\"\n",
    "    Loads and merges the three datasets used for the clustering.\n",
    "    Also ensures that the country column (which is the join key)\n",
    "    is consistently represented.\n",
    "\n",
    "    Returns:\n",
    "        df_data: Dataframe that contains all columns from the\n",
    "            original datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import EPI data\n",
    "    df_epi = pd.read_excel(\"data/epi_data2.xlsx\")\n",
    "\n",
    "    # Import COVID and socioeconomic data\n",
    "    df_covid_socio = pd.read_csv(\"data/Consolidated_COVID_Socioeconomics.csv\")\n",
    "    df_covid_socio = df_covid_socio.rename(\n",
    "        columns={col: col.lower() for col in df_covid_socio.columns}\n",
    "    )\n",
    "    df_disease = pd.read_csv(\"data/diseases.csv\")\n",
    "\n",
    "    # Ensure that countries have consistent naming by renaming\n",
    "    # countries in the COVID and socioeconomic dataset\n",
    "    country_map = {\n",
    "        \"Egypt, Arab Rep.\": \"Egypt\",\n",
    "        \"Iran, Islamic Rep.\": \"Iran\",\n",
    "        \"Iran (Islamic Republic of)\": \"Iran\",\n",
    "        \"Russian Federation\": \"Russia\",\n",
    "        \"Turkiye\": \"Turkey\",\n",
    "        \"United States\": \"United States of America\"\n",
    "    }\n",
    "    for old, new in country_map.items():\n",
    "        df_covid_socio.loc[df_covid_socio[\"country\"] == old, \"country\"] = new\n",
    "        df_disease.loc[df_disease[\"location_name\"]== old, \"location_name\"] = new\n",
    "    df_disease = df_disease.rename(columns={'location_name': 'country'})\n",
    "    # Merge the dataframes together\n",
    "    df_data = df_epi.merge(df_covid_socio, on=\"country\")\n",
    "    df_data = df_data.merge(df_disease, on=\"country\")\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c9e36d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "    \"\"\"\n",
    "    Basic preprocessing of the data incl. dropping columns, reformatting,\n",
    "    replacing NaNs and standardizing data. The logic for replacing NaNs\n",
    "    is to use the mean of each column. We don't want to drop these rows\n",
    "    (countries) altogether, and this is the least \"biased\", simple approach.\n",
    "\n",
    "    Args:\n",
    "        df: Dataframe containing the joined data from all sources, but\n",
    "            which has not been preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        df: Dataframe with the cleaned and preprocessed data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop columns that will not be used in the clustering\n",
    "    df = df.drop([\"cum_cases\", \"cum_deaths\", \"che_2019\", \"Vitamin A deficiency\", \"Dietary iron deficiency\", \"Chronic kidney disease due to diabetes mellitus type 1\", \"Chronic kidney disease due to diabetes mellitus type 2\"], axis=\"columns\")\n",
    "\n",
    "    # Create list with column names except \"country\"\n",
    "    col_names = list(df.columns)\n",
    "    col_names.remove(\"country\")\n",
    "\n",
    "    # Change format of missing values from \"-\" to np.nan\n",
    "    df = df.replace(\"-\", np.nan)\n",
    "\n",
    "    # Cast all columns to float type\n",
    "    for col in col_names:\n",
    "        df[col] = df[col].astype(float)\n",
    "\n",
    "    # Replace NaN with the mean of each column, ignoring NaNs\n",
    "    for col in col_names:\n",
    "        df[col] = df[col].fillna(np.nanmean(df[col]))\n",
    "\n",
    "    # Do standard scaling of all feature columns\n",
    "    countries = df[\"country\"]  # Save column for later use\n",
    "    data = df.drop(\"country\", axis=\"columns\")\n",
    "    data = StandardScaler().fit_transform(data)\n",
    "    df_data = pd.DataFrame(data, columns=col_names)\n",
    "\n",
    "    # Join standardized data with country labels\n",
    "    df = pd.concat([countries, df_data], axis=\"columns\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e35c2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>pm2.5_exposure</th>\n",
       "      <th>overall_epi</th>\n",
       "      <th>environ_health</th>\n",
       "      <th>air_quality</th>\n",
       "      <th>solid_fuels</th>\n",
       "      <th>sanitation_water</th>\n",
       "      <th>unsafe_water</th>\n",
       "      <th>gdp</th>\n",
       "      <th>che_2020</th>\n",
       "      <th>cum_cases_100k</th>\n",
       "      <th>cum_deaths_100k</th>\n",
       "      <th>lexp_avg</th>\n",
       "      <th>smoking_prev</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>Chronic kidney disease due to hypertension</th>\n",
       "      <th>Diabetes and kidney diseases</th>\n",
       "      <th>Lower respiratory infections</th>\n",
       "      <th>Nutritional deficiencies</th>\n",
       "      <th>Respiratory infections and tuberculosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>-0.876231</td>\n",
       "      <td>-0.058749</td>\n",
       "      <td>-1.394648</td>\n",
       "      <td>-1.215604</td>\n",
       "      <td>-1.595540</td>\n",
       "      <td>-1.087721</td>\n",
       "      <td>-1.005544</td>\n",
       "      <td>-0.842364</td>\n",
       "      <td>-7.120921e-01</td>\n",
       "      <td>-1.180212</td>\n",
       "      <td>-1.162468e+00</td>\n",
       "      <td>-1.587326</td>\n",
       "      <td>-1.141526</td>\n",
       "      <td>1.754219</td>\n",
       "      <td>-0.301219</td>\n",
       "      <td>-0.269360</td>\n",
       "      <td>-0.132512</td>\n",
       "      <td>-0.159508</td>\n",
       "      <td>-0.132512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>-0.112437</td>\n",
       "      <td>0.197457</td>\n",
       "      <td>-0.382678</td>\n",
       "      <td>-0.302744</td>\n",
       "      <td>-0.750666</td>\n",
       "      <td>-0.157708</td>\n",
       "      <td>-0.208826</td>\n",
       "      <td>-0.635441</td>\n",
       "      <td>-8.823636e-17</td>\n",
       "      <td>-0.551338</td>\n",
       "      <td>-3.361936e-01</td>\n",
       "      <td>0.396329</td>\n",
       "      <td>0.323736</td>\n",
       "      <td>0.019097</td>\n",
       "      <td>-0.402806</td>\n",
       "      <td>-0.407195</td>\n",
       "      <td>-0.281790</td>\n",
       "      <td>-0.246332</td>\n",
       "      <td>-0.281790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>-1.020134</td>\n",
       "      <td>-1.083575</td>\n",
       "      <td>-0.298347</td>\n",
       "      <td>-0.223906</td>\n",
       "      <td>0.617968</td>\n",
       "      <td>-0.186324</td>\n",
       "      <td>-0.251318</td>\n",
       "      <td>-0.708784</td>\n",
       "      <td>-6.598738e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.240361e-16</td>\n",
       "      <td>0.369062</td>\n",
       "      <td>-0.271899</td>\n",
       "      <td>1.983386</td>\n",
       "      <td>-0.113897</td>\n",
       "      <td>-0.071885</td>\n",
       "      <td>-0.176403</td>\n",
       "      <td>-0.165931</td>\n",
       "      <td>-0.176403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>0.311893</td>\n",
       "      <td>-0.241754</td>\n",
       "      <td>0.304618</td>\n",
       "      <td>0.298914</td>\n",
       "      <td>0.059914</td>\n",
       "      <td>0.225028</td>\n",
       "      <td>0.116943</td>\n",
       "      <td>-0.443312</td>\n",
       "      <td>-4.080722e-01</td>\n",
       "      <td>0.053988</td>\n",
       "      <td>9.521885e-01</td>\n",
       "      <td>0.225912</td>\n",
       "      <td>0.609640</td>\n",
       "      <td>0.150050</td>\n",
       "      <td>-0.178060</td>\n",
       "      <td>-0.202120</td>\n",
       "      <td>-0.152762</td>\n",
       "      <td>-0.160323</td>\n",
       "      <td>-0.152762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>-1.009064</td>\n",
       "      <td>0.285300</td>\n",
       "      <td>-0.353162</td>\n",
       "      <td>-0.526809</td>\n",
       "      <td>-0.058555</td>\n",
       "      <td>-0.043245</td>\n",
       "      <td>0.194844</td>\n",
       "      <td>-0.661946</td>\n",
       "      <td>-5.292153e-01</td>\n",
       "      <td>-0.348417</td>\n",
       "      <td>1.004505e+00</td>\n",
       "      <td>-0.237623</td>\n",
       "      <td>0.776418</td>\n",
       "      <td>1.230409</td>\n",
       "      <td>-0.397942</td>\n",
       "      <td>-0.402284</td>\n",
       "      <td>-0.279110</td>\n",
       "      <td>-0.245055</td>\n",
       "      <td>-0.279110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  pm2.5_exposure  overall_epi  environ_health  air_quality  \\\n",
       "0  Afghanistan       -0.876231    -0.058749       -1.394648    -1.215604   \n",
       "1      Albania       -0.112437     0.197457       -0.382678    -0.302744   \n",
       "2      Algeria       -1.020134    -1.083575       -0.298347    -0.223906   \n",
       "3    Argentina        0.311893    -0.241754        0.304618     0.298914   \n",
       "4      Armenia       -1.009064     0.285300       -0.353162    -0.526809   \n",
       "\n",
       "   solid_fuels  sanitation_water  unsafe_water       gdp      che_2020  \\\n",
       "0    -1.595540         -1.087721     -1.005544 -0.842364 -7.120921e-01   \n",
       "1    -0.750666         -0.157708     -0.208826 -0.635441 -8.823636e-17   \n",
       "2     0.617968         -0.186324     -0.251318 -0.708784 -6.598738e-01   \n",
       "3     0.059914          0.225028      0.116943 -0.443312 -4.080722e-01   \n",
       "4    -0.058555         -0.043245      0.194844 -0.661946 -5.292153e-01   \n",
       "\n",
       "   cum_cases_100k  cum_deaths_100k  lexp_avg  smoking_prev   alcohol  \\\n",
       "0       -1.180212    -1.162468e+00 -1.587326     -1.141526  1.754219   \n",
       "1       -0.551338    -3.361936e-01  0.396329      0.323736  0.019097   \n",
       "2        0.000000    -2.240361e-16  0.369062     -0.271899  1.983386   \n",
       "3        0.053988     9.521885e-01  0.225912      0.609640  0.150050   \n",
       "4       -0.348417     1.004505e+00 -0.237623      0.776418  1.230409   \n",
       "\n",
       "   Chronic kidney disease due to hypertension  Diabetes and kidney diseases  \\\n",
       "0                                   -0.301219                     -0.269360   \n",
       "1                                   -0.402806                     -0.407195   \n",
       "2                                   -0.113897                     -0.071885   \n",
       "3                                   -0.178060                     -0.202120   \n",
       "4                                   -0.397942                     -0.402284   \n",
       "\n",
       "   Lower respiratory infections  Nutritional deficiencies  \\\n",
       "0                     -0.132512                 -0.159508   \n",
       "1                     -0.281790                 -0.246332   \n",
       "2                     -0.176403                 -0.165931   \n",
       "3                     -0.152762                 -0.160323   \n",
       "4                     -0.279110                 -0.245055   \n",
       "\n",
       "   Respiratory infections and tuberculosis  \n",
       "0                                -0.132512  \n",
       "1                                -0.281790  \n",
       "2                                -0.176403  \n",
       "3                                -0.152762  \n",
       "4                                -0.279110  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load, preprocess and inspect the data\n",
    "df_data = load_and_merge_data()\n",
    "df_preprocessed = data_preprocessing(df_data)\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5fce54",
   "metadata": {},
   "source": [
    "## 2. Implementation of K-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "103116b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    \"\"\"\n",
    "    Performs K-means clustering using Lloyd's algorithm.\n",
    "\n",
    "    Attributes:\n",
    "        xxx:\n",
    "\n",
    "    Methods:\n",
    "        xxx:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters=4, max_iter=500, tol=0.0001):\n",
    "        # Hyperparameters\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.shape = None  # Placeholder for shape of input data\n",
    "\n",
    "        # Arrays for labels and centroids\n",
    "        self.labels = None\n",
    "        self.centroids = None\n",
    "        self.old_centroids = None\n",
    "\n",
    "    def initialize_centroids(self, data):\n",
    "        \"\"\"\n",
    "        For the first iteration, centroids are initialized randomly as\n",
    "        one of the points among the input data.\n",
    "        \"\"\"\n",
    "        # Select K points from the data as initial centroids\n",
    "        self.centroids = data[np.random.choice(self.shape[0], size=self.n_clusters)]\n",
    "\n",
    "        # Initialize array for storing centroids from the last iteration\n",
    "        self.old_centroids = np.zeros((self.n_clusters, self.shape[1]))\n",
    "\n",
    "    def calculate_centroids(self, data):\n",
    "        \"\"\"\n",
    "        Calculates new centroids given the latest cluster assignments.\n",
    "        The centroids is the mean of all the points in a cluster. If there\n",
    "        are no points assigned to a cluster, this cluster centroid is set\n",
    "        to the point furthest away from the current centroid.\n",
    "        \"\"\"\n",
    "        # Iterate through each centroids\n",
    "        for label in range(self.n_clusters):\n",
    "            # If the cluster is not empty, use mean as the new centroid\n",
    "            if len(data[self.labels == label]) > 0:\n",
    "                self.centroids[label, :] = np.mean(data[self.labels == label], axis=0)\n",
    "\n",
    "            # Otherwise, use the outlier logic described above\n",
    "            else:\n",
    "                outlier_idx = self.find_largest_outlier(data, self.centroids[label, :])\n",
    "                self.centroids[label, :] = data[outlier_idx]\n",
    "\n",
    "    def assign_clusters(self, data):\n",
    "        \"\"\"\n",
    "        Calculate the distance between each data point and each centroids.\n",
    "        Assign labels to each point based on the closest centroid.\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate one array with distances for each centroid\n",
    "        distance_arrays = []\n",
    "        for centroid in self.centroids:\n",
    "            distances = np.sqrt(np.sum(np.power(data - centroid, 2), axis=1))\n",
    "            distance_arrays.append(distances.reshape(-1, 1))\n",
    "\n",
    "        # Stack all distance arrays into a matrix with one row for each\n",
    "        # country and one column for each centroid\n",
    "        distance_matrix = np.concatenate(distance_arrays, axis=1)\n",
    "\n",
    "        # Find the label of the closest centroid\n",
    "        self.labels = np.argmin(distance_matrix, axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_largest_outlier(data, centroid):\n",
    "        \"\"\"Find the point furthest away from a given cluster centroid.\"\"\"\n",
    "        distances = np.sqrt(np.sum(np.power(data - centroid, 2), axis=1))\n",
    "        outlier_idx = np.argmax(distances)\n",
    "        return outlier_idx\n",
    "\n",
    "    def fit_predict(self, data):\n",
    "        \"\"\"\n",
    "        Runs the clustering algorithm on the input data and returns\n",
    "        cluster labels.\n",
    "        \"\"\"\n",
    "        # If data comes as a dataframe, convert to numpy array\n",
    "        if type(data) == pd.DataFrame:\n",
    "            data = data.to_numpy()\n",
    "\n",
    "        # Initialize centroids and labels\n",
    "        self.shape = data.shape\n",
    "        self.initialize_centroids(data)\n",
    "        cluster_labels = np.zeros((self.shape[0],))\n",
    "\n",
    "        # Loop until tolerance is met, or until reaching max iterations\n",
    "        iterations = 0\n",
    "        while (\n",
    "            np.all(np.abs(self.centroids - self.old_centroids)) > self.tol\n",
    "            or iterations < self.max_iter\n",
    "        ):\n",
    "            iterations += 1\n",
    "\n",
    "            # Assign each point to a cluster\n",
    "            self.assign_clusters(data)\n",
    "\n",
    "            # Re-calculate the centroids\n",
    "            self.old_centroids = np.copy(self.centroids)\n",
    "            self.calculate_centroids(data)\n",
    "\n",
    "        # Return the final labels\n",
    "        return self.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a5510",
   "metadata": {},
   "source": [
    "## 3. Performing clustering and analyzing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02c5be9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 3, 3, 1, 0, 0, 2, 0, 1, 3, 0, 3, 1, 3, 2, 0, 0, 3, 1, 2,\n",
       "       0, 0, 0, 2, 1, 1, 2, 2, 2, 3, 1, 2, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       2, 0, 2, 0, 0, 2, 1, 2, 1, 0, 2, 0, 2, 3, 1, 0, 3, 3, 0, 2, 3, 1,\n",
       "       0, 1, 2, 1, 1, 3, 3, 0, 1, 1, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the data to be used\n",
    "data = df_preprocessed.drop(\"country\", axis=\"columns\").to_numpy()\n",
    "\n",
    "# Instantiate model object with hyperparameters\n",
    "kmeans = KMeans(n_clusters=4, max_iter=1000, tol=0.0001)\n",
    "\n",
    "# Do fit and predict cluster labels\n",
    "result = kmeans.fit_predict(data)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60d48dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ['Albania', 'Algeria', 'Azerbaijan', 'Bahrain', 'Belarus', 'Brazil', 'Colombia', 'Costa Rica', 'Dominican Republic', 'Ecuador', 'Egypt', 'Iran', 'Iraq', 'Kazakhstan', 'Kuwait', 'Malaysia', 'Mexico', 'Morocco', 'Oman', 'Panama', 'Qatar', 'Saudi Arabia', 'South Africa', 'United Arab Emirates', 'Uzbekistan']\n",
      "\n",
      "1: ['Austria', 'Belgium', 'Canada', 'Denmark', 'France', 'Germany', 'Iceland', 'Ireland', 'Israel', 'Italy', 'Japan', 'Luxembourg', 'Netherlands', 'Norway', 'Portugal', 'Singapore', 'Spain', 'Sweden', 'Switzerland', 'United Kingdom', 'United States of America']\n",
      "\n",
      "2: ['Afghanistan', 'Bangladesh', 'China', 'Djibouti', 'Ethiopia', 'Ghana', 'Guatemala', 'Honduras', 'India', 'Indonesia', 'Madagascar', 'Mauritania', 'Nepal', 'Nigeria', 'Pakistan', 'Philippines', 'Senegal', 'Sudan', 'Zambia']\n",
      "\n",
      "3: ['Argentina', 'Armenia', 'Bosnia and Herzegovina', 'Bulgaria', 'Chile', 'Croatia', 'Hungary', 'Poland', 'Romania', 'Russia', 'Serbia', 'Turkey', 'Ukraine']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Create output dataframe\n",
    "output = zip(list(df_preprocessed[\"country\"].values), result)\n",
    "df_output = pd.DataFrame(list(output), columns=[\"country\", \"cluster\"])\n",
    "df_output = df_output.sort_values(\"country\")\n",
    "\n",
    "# List with all countries per cluster\n",
    "for label in range(4):\n",
    "    country_list = list(df_output.loc[df_output[\"cluster\"] == label][\"country\"].values)\n",
    "    print(f\"{label}: {country_list}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead793e9",
   "metadata": {},
   "source": [
    "### 3.1 Benchmarking against sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f9ee10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 3, 3, 3, 0, 1, 3, 1, 3, 0, 3, 3, 3, 0, 3, 2, 3, 3, 3, 0, 1,\n",
       "       1, 3, 1, 1, 0, 0, 1, 1, 1, 3, 0, 2, 1, 3, 1, 0, 0, 0, 0, 3, 3, 0,\n",
       "       1, 3, 1, 3, 1, 1, 0, 1, 0, 1, 1, 3, 1, 3, 0, 3, 3, 3, 3, 1, 3, 0,\n",
       "       1, 0, 1, 0, 0, 3, 3, 3, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans as SKLKMeans\n",
    "\n",
    "kmeans_comp = SKLKMeans(\n",
    "    n_clusters=4, init=\"random\", n_init=\"auto\", max_iter=1000, tol=0.0001\n",
    ")\n",
    "res_comp = kmeans_comp.fit_predict(data)\n",
    "res_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c65c23f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ['Austria', 'Belgium', 'Canada', 'Denmark', 'France', 'Germany', 'Iceland', 'Ireland', 'Israel', 'Italy', 'Japan', 'Luxembourg', 'Netherlands', 'Norway', 'Portugal', 'Singapore', 'Spain', 'Sweden', 'Switzerland', 'United Kingdom', 'United States of America']\n",
      "\n",
      "1: ['Afghanistan', 'Azerbaijan', 'Bangladesh', 'Djibouti', 'Dominican Republic', 'Egypt', 'Ethiopia', 'Ghana', 'Guatemala', 'Honduras', 'Indonesia', 'Iraq', 'Madagascar', 'Mauritania', 'Morocco', 'Nepal', 'Nigeria', 'Oman', 'Pakistan', 'Philippines', 'Senegal', 'South Africa', 'Sudan', 'Uzbekistan', 'Zambia']\n",
      "\n",
      "2: ['China', 'India']\n",
      "\n",
      "3: ['Albania', 'Algeria', 'Argentina', 'Armenia', 'Bahrain', 'Belarus', 'Bosnia and Herzegovina', 'Brazil', 'Bulgaria', 'Chile', 'Colombia', 'Costa Rica', 'Croatia', 'Ecuador', 'Hungary', 'Iran', 'Kazakhstan', 'Kuwait', 'Malaysia', 'Mexico', 'Panama', 'Poland', 'Qatar', 'Romania', 'Russia', 'Saudi Arabia', 'Serbia', 'Turkey', 'Ukraine', 'United Arab Emirates']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create output dataframe\n",
    "output_comp = zip(list(df_preprocessed[\"country\"].values), res_comp)\n",
    "df_comp = pd.DataFrame(list(output_comp), columns=[\"country\", \"cluster\"])\n",
    "df_comp = df_comp.sort_values(\"country\")\n",
    "\n",
    "# List with all countries per cluster\n",
    "for label in range(4):\n",
    "    country_list = list(df_comp.loc[df_comp[\"cluster\"] == label][\"country\"].values)\n",
    "    print(f\"{label}: {country_list}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1142dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bc69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9f0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
