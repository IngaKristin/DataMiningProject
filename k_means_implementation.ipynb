{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90bcd6aa",
   "metadata": {},
   "source": [
    "# Data Mining project\n",
    "\n",
    "### Clustering of countries for COVID-19 cases based on disease prevalence, health systems and environmental indicators\n",
    "\n",
    "Authors: Inga Wohlert, Nicolas Pablo Viola, Jakob Nystr√∂m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04f32b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d63384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jupyter_black\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8127bfb0",
   "metadata": {},
   "source": [
    "## 1. Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8914752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_data():\n",
    "    \"\"\"\n",
    "    Loads and merges the three datasets used for the clustering.\n",
    "    Also ensures that the country column (which is the join key)\n",
    "    is consistently represented.\n",
    "\n",
    "    Returns:\n",
    "        df_data: Dataframe that contains all columns from the\n",
    "            original datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import EPI data\n",
    "    df_epi = pd.read_excel(\"epi_data2.xlsx\")\n",
    "\n",
    "    # Import COVID and socioeconomic data\n",
    "    df_covid_socio = pd.read_csv(\"Consolidated_COVID_Socioeconomics.csv\")\n",
    "    df_covid_socio = df_covid_socio.rename(\n",
    "        columns={col: col.lower() for col in df_covid_socio.columns}\n",
    "    )\n",
    "\n",
    "    # TODO: Add the final dataset with health data\n",
    "\n",
    "    # Ensure that countries have consistent naming by renaming\n",
    "    # countries in the COVID and socioeconomic dataset\n",
    "    country_map = {\n",
    "        \"Egypt, Arab Rep.\": \"Egypt\",\n",
    "        \"Iran, Islamic Rep.\": \"Iran\",\n",
    "        \"Russian Federation\": \"Russia\",\n",
    "        \"Turkiye\": \"Turkey\",\n",
    "        \"United States\": \"United States of America\",\n",
    "    }\n",
    "    for old, new in country_map.items():\n",
    "        df_covid_socio.loc[df_covid_socio[\"country\"] == old, \"country\"] = new\n",
    "\n",
    "    # Merge the dataframes together\n",
    "    df_data = df_epi.merge(df_covid_socio, on=\"country\")\n",
    "\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c9e36d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "    \"\"\"\n",
    "    Basic preprocessing of the data incl. dropping columns, reformatting,\n",
    "    replacing NaNs and standardizing data. The logic for replacing NaNs\n",
    "    is to use the mean of each column. We don't want to drop these rows\n",
    "    (countries) altogether, and this is the least \"biased\", simple approach.\n",
    "\n",
    "    Args:\n",
    "        df: Dataframe containing the joined data from all sources, but\n",
    "            which has not been preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        df: Dataframe with the cleaned and preprocessed data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop columns that will not be used in the clustering\n",
    "    df = df.drop([\"cum_cases\", \"cum_deaths\", \"che_2019\"], axis=\"columns\")\n",
    "\n",
    "    # Create list with column names except \"country\"\n",
    "    col_names = list(df.columns)\n",
    "    col_names.remove(\"country\")\n",
    "\n",
    "    # Change format of missing values from \"-\" to np.nan\n",
    "    df = df.replace(\"-\", np.nan)\n",
    "\n",
    "    # Cast all columns to float type\n",
    "    for col in col_names:\n",
    "        df[col] = df[col].astype(float)\n",
    "\n",
    "    # Replace NaN with the mean of each column, ignoring NaNs\n",
    "    for col in col_names:\n",
    "        df[col] = df[col].fillna(np.nanmean(df[col]))\n",
    "\n",
    "    # Do standard scaling of all feature columns\n",
    "    countries = df[\"country\"]  # Save column for later use\n",
    "    data = df.drop(\"country\", axis=\"columns\")\n",
    "    data = StandardScaler().fit_transform(data)\n",
    "    df_data = pd.DataFrame(data, columns=col_names)\n",
    "\n",
    "    # Join standardized data with country labels\n",
    "    df = pd.concat([countries, df_data], axis=\"columns\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e35c2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>pm2.5_exposure</th>\n",
       "      <th>overall_epi</th>\n",
       "      <th>environ_health</th>\n",
       "      <th>air_quality</th>\n",
       "      <th>solid_fuels</th>\n",
       "      <th>sanitation_water</th>\n",
       "      <th>unsafe_water</th>\n",
       "      <th>gdp</th>\n",
       "      <th>che_2020</th>\n",
       "      <th>cum_cases_100k</th>\n",
       "      <th>cum_deaths_100k</th>\n",
       "      <th>lexp_avg</th>\n",
       "      <th>smoking_prev</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>-0.873488</td>\n",
       "      <td>-0.055078</td>\n",
       "      <td>-1.393668</td>\n",
       "      <td>-1.212660</td>\n",
       "      <td>-1.584896</td>\n",
       "      <td>-1.083314</td>\n",
       "      <td>-1.001500</td>\n",
       "      <td>-0.835658</td>\n",
       "      <td>-7.062674e-01</td>\n",
       "      <td>-1.176035</td>\n",
       "      <td>-1.172313</td>\n",
       "      <td>-1.562401</td>\n",
       "      <td>-1.133309</td>\n",
       "      <td>1.769052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>-0.106044</td>\n",
       "      <td>0.202604</td>\n",
       "      <td>-0.377245</td>\n",
       "      <td>-0.296152</td>\n",
       "      <td>-0.739347</td>\n",
       "      <td>-0.149916</td>\n",
       "      <td>-0.201677</td>\n",
       "      <td>-0.628110</td>\n",
       "      <td>-8.856398e-17</td>\n",
       "      <td>-0.544690</td>\n",
       "      <td>-0.340946</td>\n",
       "      <td>0.411334</td>\n",
       "      <td>0.335265</td>\n",
       "      <td>0.026634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>-1.018079</td>\n",
       "      <td>-1.085805</td>\n",
       "      <td>-0.292543</td>\n",
       "      <td>-0.216999</td>\n",
       "      <td>0.630380</td>\n",
       "      <td>-0.178636</td>\n",
       "      <td>-0.244334</td>\n",
       "      <td>-0.701674</td>\n",
       "      <td>-6.538551e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384204</td>\n",
       "      <td>-0.261717</td>\n",
       "      <td>1.999183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>0.320315</td>\n",
       "      <td>-0.239136</td>\n",
       "      <td>0.313075</td>\n",
       "      <td>0.307911</td>\n",
       "      <td>0.071881</td>\n",
       "      <td>0.234213</td>\n",
       "      <td>0.125362</td>\n",
       "      <td>-0.435399</td>\n",
       "      <td>-4.011186e-01</td>\n",
       "      <td>0.063013</td>\n",
       "      <td>0.955375</td>\n",
       "      <td>0.241770</td>\n",
       "      <td>0.621816</td>\n",
       "      <td>0.158137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>-1.006957</td>\n",
       "      <td>0.290952</td>\n",
       "      <td>-0.347599</td>\n",
       "      <td>-0.521113</td>\n",
       "      <td>-0.046683</td>\n",
       "      <td>-0.035036</td>\n",
       "      <td>0.203567</td>\n",
       "      <td>-0.654695</td>\n",
       "      <td>-5.227115e-01</td>\n",
       "      <td>-0.340972</td>\n",
       "      <td>1.008014</td>\n",
       "      <td>-0.219447</td>\n",
       "      <td>0.788970</td>\n",
       "      <td>1.243039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  pm2.5_exposure  overall_epi  environ_health  air_quality  \\\n",
       "0  Afghanistan       -0.873488    -0.055078       -1.393668    -1.212660   \n",
       "1      Albania       -0.106044     0.202604       -0.377245    -0.296152   \n",
       "2      Algeria       -1.018079    -1.085805       -0.292543    -0.216999   \n",
       "3    Argentina        0.320315    -0.239136        0.313075     0.307911   \n",
       "4      Armenia       -1.006957     0.290952       -0.347599    -0.521113   \n",
       "\n",
       "   solid_fuels  sanitation_water  unsafe_water       gdp      che_2020  \\\n",
       "0    -1.584896         -1.083314     -1.001500 -0.835658 -7.062674e-01   \n",
       "1    -0.739347         -0.149916     -0.201677 -0.628110 -8.856398e-17   \n",
       "2     0.630380         -0.178636     -0.244334 -0.701674 -6.538551e-01   \n",
       "3     0.071881          0.234213      0.125362 -0.435399 -4.011186e-01   \n",
       "4    -0.046683         -0.035036      0.203567 -0.654695 -5.227115e-01   \n",
       "\n",
       "   cum_cases_100k  cum_deaths_100k  lexp_avg  smoking_prev   alcohol  \n",
       "0       -1.176035        -1.172313 -1.562401     -1.133309  1.769052  \n",
       "1       -0.544690        -0.340946  0.411334      0.335265  0.026634  \n",
       "2        0.000000         0.000000  0.384204     -0.261717  1.999183  \n",
       "3        0.063013         0.955375  0.241770      0.621816  0.158137  \n",
       "4       -0.340972         1.008014 -0.219447      0.788970  1.243039  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load, preprocess and inspect the data\n",
    "df_data = load_and_merge_data()\n",
    "df_preprocessed = data_preprocessing(df_data)\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5fce54",
   "metadata": {},
   "source": [
    "## 2. Implementation of K-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "103116b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    \"\"\"\n",
    "    Performs K-means clustering using Lloyd's algorithm.\n",
    "\n",
    "    Attributes:\n",
    "        xxx:\n",
    "\n",
    "    Methods:\n",
    "        xxx:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters=4, max_iter=500, tol=0.0001):\n",
    "        # Hyperparameters\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.shape = None  # Placeholder for shape of input data\n",
    "\n",
    "        # Arrays for labels and centroids\n",
    "        self.labels = None\n",
    "        self.centroids = None\n",
    "        self.old_centroids = None\n",
    "\n",
    "    def initialize_centroids(self, data):\n",
    "        \"\"\"\n",
    "        For the first iteration, centroids are initialized randomly as\n",
    "        one of the points among the input data.\n",
    "        \"\"\"\n",
    "        # Select K points from the data as initial centroids\n",
    "        self.centroids = data[np.random.choice(self.shape[0], size=self.n_clusters)]\n",
    "\n",
    "        # Initialize array for storing centroids from the last iteration\n",
    "        self.old_centroids = np.zeros((self.n_clusters, self.shape[1]))\n",
    "\n",
    "    def calculate_centroids(self, data):\n",
    "        \"\"\"\n",
    "        Calculates new centroids given the latest cluster assignments.\n",
    "        The centroids is the mean of all the points in a cluster. If there\n",
    "        are no points assigned to a cluster, this cluster centroid is set\n",
    "        to the point furthest away from the current centroid.\n",
    "        \"\"\"\n",
    "        # Iterate through each centroids\n",
    "        for label in range(self.n_clusters):\n",
    "            # If the cluster is not empty, use mean as the new centroid\n",
    "            if len(data[self.labels == label]) > 0:\n",
    "                self.centroids[label, :] = np.mean(data[self.labels == label], axis=0)\n",
    "\n",
    "            # Otherwise, use the outlier logic described above\n",
    "            else:\n",
    "                outlier_idx = self.find_largest_outlier(data, self.centroids[label, :])\n",
    "                self.centroids[label, :] = data[outlier_idx]\n",
    "\n",
    "    def assign_clusters(self, data):\n",
    "        \"\"\"\n",
    "        Calculate the distance between each data point and each centroids.\n",
    "        Assign labels to each point based on the closest centroid.\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate one array with distances for each centroid\n",
    "        distance_arrays = []\n",
    "        for centroid in self.centroids:\n",
    "            distances = np.sqrt(np.sum(np.power(data - centroid, 2), axis=1))\n",
    "            distance_arrays.append(distances.reshape(-1, 1))\n",
    "\n",
    "        # Stack all distance arrays into a matrix with one row for each\n",
    "        # country and one column for each centroid\n",
    "        distance_matrix = np.concatenate(distance_arrays, axis=1)\n",
    "\n",
    "        # Find the label of the closest centroid\n",
    "        self.labels = np.argmin(distance_matrix, axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_largest_outlier(data, centroid):\n",
    "        \"\"\"Find the point furthest away from a given cluster centroid.\"\"\"\n",
    "        distances = np.sqrt(np.sum(np.power(data - centroid, 2), axis=1))\n",
    "        outlier_idx = np.argmax(distances)\n",
    "        return outlier_idx\n",
    "\n",
    "    def fit_predict(self, data):\n",
    "        \"\"\"\n",
    "        Runs the clustering algorithm on the input data and returns\n",
    "        cluster labels.\n",
    "        \"\"\"\n",
    "        # If data comes as a dataframe, convert to numpy array\n",
    "        if type(data) == pd.DataFrame:\n",
    "            data = data.to_numpy()\n",
    "\n",
    "        # Initialize centroids and labels\n",
    "        self.shape = data.shape\n",
    "        self.initialize_centroids(data)\n",
    "        cluster_labels = np.zeros((self.shape[0],))\n",
    "\n",
    "        # Loop until tolerance is met, or until reaching max iterations\n",
    "        iterations = 0\n",
    "        while (\n",
    "            np.all(np.abs(self.centroids - self.old_centroids)) > self.tol\n",
    "            or iterations < self.max_iter\n",
    "        ):\n",
    "            iterations += 1\n",
    "\n",
    "            # Assign each point to a cluster\n",
    "            self.assign_clusters(data)\n",
    "\n",
    "            # Re-calculate the centroids\n",
    "            self.old_centroids = np.copy(self.centroids)\n",
    "            self.calculate_centroids(data)\n",
    "\n",
    "        # Return the final labels\n",
    "        return self.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a5510",
   "metadata": {},
   "source": [
    "## 3. Performing clustering and analyzing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "02c5be9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 2, 2, 3, 1, 1, 0, 1, 3, 0, 2, 2, 2, 3, 2, 1, 1, 1, 2, 3,\n",
       "       0, 1, 1, 1, 0, 3, 3, 0, 0, 0, 2, 3, 0, 0, 1, 1, 3, 3, 3, 3, 1, 1,\n",
       "       3, 0, 1, 0, 1, 1, 0, 3, 0, 3, 1, 0, 1, 0, 2, 3, 1, 2, 2, 1, 0, 2,\n",
       "       3, 0, 3, 0, 3, 3, 2, 2, 1, 3, 3, 1, 0])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the data to be used\n",
    "data = df_preprocessed.drop(\"country\", axis=\"columns\").to_numpy()\n",
    "\n",
    "# Instantiate model object with hyperparameters\n",
    "kmeans = KMeans(n_clusters=4, max_iter=1000, tol=0.0001)\n",
    "\n",
    "# Do fit and predict cluster labels\n",
    "result = kmeans.fit_predict(data)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "60d48dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ['Afghanistan', 'Bangladesh', 'Bolivia', 'Djibouti', 'Ethiopia', 'Ghana', 'Guatemala', 'Honduras', 'India', 'Indonesia', 'Madagascar', 'Mauritania', 'Nepal', 'Nigeria', 'Pakistan', 'Philippines', 'Senegal', 'South Africa', 'Sudan', 'Zambia']\n",
      "\n",
      "1: ['Albania', 'Algeria', 'Azerbaijan', 'Bahrain', 'Belarus', 'China', 'Colombia', 'Costa Rica', 'Dominican Republic', 'Ecuador', 'Egypt', 'Iran', 'Iraq', 'Kazakhstan', 'Kuwait', 'Malaysia', 'Mexico', 'Morocco', 'Oman', 'Panama', 'Qatar', 'Saudi Arabia', 'United Arab Emirates', 'Uzbekistan']\n",
      "\n",
      "2: ['Argentina', 'Armenia', 'Bosnia and Herzegovina', 'Brazil', 'Bulgaria', 'Chile', 'Croatia', 'Hungary', 'Poland', 'Romania', 'Russia', 'Serbia', 'Turkey', 'Ukraine']\n",
      "\n",
      "3: ['Austria', 'Belgium', 'Canada', 'Denmark', 'France', 'Germany', 'Iceland', 'Ireland', 'Israel', 'Italy', 'Japan', 'Luxembourg', 'Netherlands', 'Norway', 'Portugal', 'Singapore', 'Spain', 'Sweden', 'Switzerland', 'United Kingdom', 'United States of America']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Create output dataframe\n",
    "output = zip(list(df_preprocessed[\"country\"].values), result)\n",
    "df_output = pd.DataFrame(list(output), columns=[\"country\", \"cluster\"])\n",
    "df_output = df_output.sort_values(\"country\")\n",
    "\n",
    "# List with all countries per cluster\n",
    "for label in range(4):\n",
    "    country_list = list(df_output.loc[df_output[\"cluster\"] == label][\"country\"].values)\n",
    "    print(f\"{label}: {country_list}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead793e9",
   "metadata": {},
   "source": [
    "### 3.1 Benchmarking against sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3f9ee10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 3, 3, 0, 1, 2, 1, 2, 0, 1, 3, 3, 3, 0, 3, 2, 2, 2, 3, 0,\n",
       "       1, 2, 2, 2, 1, 0, 0, 1, 1, 1, 3, 0, 1, 1, 2, 2, 0, 0, 0, 0, 2, 2,\n",
       "       0, 1, 2, 1, 2, 2, 1, 0, 1, 0, 2, 1, 2, 1, 3, 0, 2, 3, 3, 2, 1, 3,\n",
       "       0, 1, 0, 1, 0, 0, 2, 3, 2, 0, 0, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans as SKLKMeans\n",
    "\n",
    "kmeans_comp = SKLKMeans(\n",
    "    n_clusters=4, init=\"random\", n_init=\"auto\", max_iter=1000, tol=0.0001\n",
    ")\n",
    "res_comp = kmeans_comp.fit_predict(data)\n",
    "res_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c65c23f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ['Austria', 'Belgium', 'Canada', 'Denmark', 'France', 'Germany', 'Iceland', 'Ireland', 'Israel', 'Italy', 'Japan', 'Luxembourg', 'Netherlands', 'Norway', 'Portugal', 'Singapore', 'Spain', 'Sweden', 'Switzerland', 'United Kingdom', 'United States of America']\n",
      "\n",
      "1: ['Afghanistan', 'Azerbaijan', 'Bangladesh', 'Bolivia', 'Djibouti', 'Ethiopia', 'Ghana', 'Guatemala', 'Honduras', 'India', 'Indonesia', 'Madagascar', 'Mauritania', 'Nepal', 'Nigeria', 'Pakistan', 'Philippines', 'Senegal', 'South Africa', 'Sudan', 'Uzbekistan', 'Zambia']\n",
      "\n",
      "2: ['Albania', 'Algeria', 'Bahrain', 'Belarus', 'China', 'Colombia', 'Costa Rica', 'Dominican Republic', 'Ecuador', 'Egypt', 'Iran', 'Iraq', 'Kazakhstan', 'Kuwait', 'Malaysia', 'Mexico', 'Morocco', 'Oman', 'Panama', 'Qatar', 'Saudi Arabia', 'Turkey', 'United Arab Emirates']\n",
      "\n",
      "3: ['Argentina', 'Armenia', 'Bosnia and Herzegovina', 'Brazil', 'Bulgaria', 'Chile', 'Croatia', 'Hungary', 'Poland', 'Romania', 'Russia', 'Serbia', 'Ukraine']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create output dataframe\n",
    "output_comp = zip(list(df_preprocessed[\"country\"].values), res_comp)\n",
    "df_comp = pd.DataFrame(list(output_comp), columns=[\"country\", \"cluster\"])\n",
    "df_comp = df_comp.sort_values(\"country\")\n",
    "\n",
    "# List with all countries per cluster\n",
    "for label in range(4):\n",
    "    country_list = list(df_comp.loc[df_comp[\"cluster\"] == label][\"country\"].values)\n",
    "    print(f\"{label}: {country_list}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1142dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bc69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9f0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining",
   "language": "python",
   "name": "data_mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
